Smells:

Username:
Suspected patterns:
1:
<input type=email data-codeception-id=login-email-input name=email>
ancestor data-hypernova-key="login_main_page"
sibling <header class="LoginHeaderWide">
ancestor <section class="LoginPage">
<form class="LoginCreate-loginForm" action=".../account"

5:
<form class="cia-sign-on-form" name="ciaSignOn">
linked label contents: "Email Address"
<input type=email>
<input name="email_CQa3I5u48cEghgJtNSqAMYpoRXZg40H2dgVpOxgMRQjn%2F%2BGnnK65YhkHLdR6kfkJ">
near and below in markup <h1 class="cia-sign-on__heading">Sign In to BestBuy.com</h1>

26:
<input name="username" id="input_username">
ancestor <div class="login_row">
ancestor <div class="loginbox">
ancestor <div class="loginbox_left">
ancestor <div class="loginbox_content">
near and below in markup <h2>Sign in</h2>
<form id="login_form" name="logon">

37:
<input name="loginName" data-auid="loginPage_field_LoginName">
linked label contents: "Login name or email"
<form name="loginForm" action=".../account"
ancestor <div id="checkoutLogin">
ancestor <div class="normal_login">

119:
<input id="username" name="username" for="username" title="Please enter your username">
linked label contents: "Username"
ancestor <div class="cssmod_usernameField_234234">
<form id="loginForm" class="member-login-form">
ancestor <div class="cssmod_memberLogin_sdf987">
ancestor <div class="cssmod_loginWrapper_sjd987 loginWrapper">

Actual patterns observed so far:
KEYWORDS = email, login, sign-on, signOn, username
√ <input name; id; title; or, fairly frequently, other attrs> contains KEYWORDS
√ ancestor classes and IDs (and, rarely, other attrs) contain KEYWORDS
√ <input type=email> Made it worse. Dropped per-tag accuracy from .85 to .83. Got a negative weight. I don't think this is a good smell by itself, because some username fields are meant to take emails, and others aren't. May reintroduce this if we add hidden layers so it can be part of a boolean computation.
<form action="...../account">
<form attrs=KEYWORDS>
linked label contents: KEYWORDS
near and below <hN> containing KEYWORDS

Try with and without one feature per keyword.

254:
Gets the wrong email field: a hidden one from a hidden "subscribe" popup.
Add a smell for visibility.
Maybe break off email as a separate smell, separate from other keywords.

440:
Has a concerningly low confidence, though it succeeds pagewise.

Added ancestor keyword rules.

37:
It's picking some phone number field from inside a hidden "Call us" pulldown. Why isn't it skipping the invisible stuff?
222, 256:
Grabbing the search field for no reason. Login field looks like it should be yummier.

Reverted ancestor keyword rules for now.

isVisible was calling offscreen elements visible. Fixed that. That reduced our universe of input tags. Page accuracy fell from 95% to 90%, puzzlingly. 146 is the new failure, but it doesn't fail in actual Fathom. 452 is the other failure. The correct target node has a tie for first place. I'm going to add 20 more samples so I have more than 1 or 2 failures to inform me and to give us a broader base to decide whether things really improve or hurt results or not.

At 40 samples, I notice that 146 and 187 have no target=true candidates. Let's fix that. Ah, isVisible was wrongly excluding things on 146 that were above the fold. (Don't ask me why it's loading scrolled in only 1 of my browsers.) Fixed it to use absolute coords rather than viewport-relative ones. Removed the tests against things being off the right and bottom of the page for now, because I'm not sure how to test that the window isn't legitimately super-wide or at least scrollable.
Fixing isVisible made 146 and 187 pass.

Next, is there anything obvious up with 395 and 452? If so, fix. If not, get more samples and search for the next signal to implement.

395 is picking an unrelated email subscription field. Trying breaking out email keyword as a separate smell.
Damn!
Suddenly 95→100% on the first 40 samples. Went from a mix of confidences hovering around 50-75% to basically 100% across the board, and all it took was one simple change: having it consider "email"ish keywords as separate features from "login"ish.

Pretty soon, I'm going to need a way of running precomputed coeffs using the CLI trainer.

Validated on 19 samples (see emailSplitWithLoginValidation run). 100% training, 100% validation. Big overfit: best validation loss was actually at step 47. The coeffs still wiggle around more (when I add new samples) than I would expect if I were really closing in on a global fit.